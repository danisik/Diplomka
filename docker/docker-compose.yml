version: '3.7'

services:
  elasticsearch:
    image: elasticsearch:8.2.0
    container_name: elasticsearch
    restart: always
    ports:
      - 9200:9200
      - 9300:9300
    environment:
      discovery.type: single-node
      xpack.security.enabled: false
      http.cors.enabled: true
      http.cors.allow-origin: "http://localhost:8080"
      http.cors.allow-headers: X-Requested-With,Content-Type,Content-Length,Authorization
    ulimits:
      memlock:
        soft: -1
        hard: -1
    cap_add:
      - IPC_LOCK

  elasticvue:
    image: cars10/elasticvue:0.39.0
    restart: always
    container_name: elasticvue
    ports:
      - 8080:8080

  mongo-setup:
    container_name: mongo-setup
    image: mongo:5.0.6
    restart: on-failure
    networks:
      default:
    volumes:
      - ./init/mongo/mongo_init.sh:/mongo_init.sh
      - ./init/mongo/create_mongo_database.js:/create_mongo_database.js
    entrypoint: [ "/mongo_init.sh" ]
    depends_on:
      - mongo

  mongo:
    image: mongo:5.0.6
    restart: always
    container_name: mongo
    ports:
      - 27017:27017
    environment:
      MONGO_INITDB_ROOT_USERNAME: admin
      MONGO_INITDB_ROOT_PASSWORD: password
    command: [ "/usr/bin/mongod", "--bind_ip_all", "--replSet", "rs0" ]

  mongo-express:
    image: mongo-express:0.54.0
    container_name: mongo-express
    restart: always
    ports:
      - 8081:8081
    environment:
      ME_CONFIG_MONGODB_ADMINUSERNAME: admin
      ME_CONFIG_MONGODB_ADMINPASSWORD: password
      ME_CONFIG_MONGODB_URL: mongo://admin:password@mongo:27017/

  zookeeper:
    image: confluentinc/cp-zookeeper:6.1.0
    hostname: zookeeper
    container_name: zookeeper
    restart: always
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  broker:
    image: confluentinc/cp-kafka:6.1.0
    hostname: broker
    container_name: broker
    restart: always
    depends_on:
      - zookeeper
    ports:
      - "29092:29092"
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_PUBLIC:PLAINTEXT,LISTENER_INTERNAL:PLAINTEXT
      KAFKA_LISTENERS: LISTENER_PUBLIC://broker:29092,LISTENER_INTERNAL://localhost:9092
      KAFKA_ADVERTISED_LISTENERS: LISTENER_PUBLIC://broker:29092,LISTENER_INTERNAL://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_PUBLIC
      CONFLUENT_METRICS_ENABLE: 'false'
      KAFKA_TOOLS_LOG4J_LOGLEVEL: ERROR
      KAFKA_LOG4J_LOGGERS: "kafka.controller=WARN,kafka.foo.bar=DEBUG"
      KAFKA_LOG4J_ROOT_LOGLEVEL: WARN
      KAFKA_MESSAGE_MAX_BYTES: 100000000
      KAFKA_MAX_MESSAGE_BYTES: 100000000

  connect:
    image: 'confluentinc/cp-kafka-connect:6.1.0'
    hostname: connect
    container_name: connect
    restart: always
    ports:
      - "28082:28082"
    depends_on:
      - zookeeper
      - broker
    command:
            - bash
            - -c
            - |
                echo "Installing connector plugins"
                confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.4.0
                confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:latest
                # Launch the Kafka Connect worker
                /etc/confluent/docker/run &
                # Don't exit
                sleep infinity
    environment:
      CONNECT_BOOTSTRAP_SERVERS: broker:29092
      CONNECT_REST_PORT: 28082
      CONNECT_GROUP_ID: "searcheg"
      CONNECT_CONFIG_STORAGE_TOPIC: searcheg-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: searcheg-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: searcheg-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "connect"
      CONNECT_PLUGIN_PATH: /usr/share/confluent-hub-components
      CONNECT_CONSUMER_OVERRIDE_MAX_POLL_RECORDS: 2000
      CONNECT_PRODUCER_MAX_REQUEST_SIZE: 100000000
      CONNECT_CONSUMER_FETCH_MAX_BYTES: 100000000


  connect-setup:
    hostname: connect-setup
    container_name: connect-setup
    restart: on-failure
    build: .
    networks:
      default:
    volumes:
      - ./init/connectors/elasticsearch_sink_connector.txt:/elasticsearch_sink_connector.txt
      - ./init/connectors/mongo_source_connector.txt:/mongo_source_connector.txt
      - ./init/connectors/connect.sh:/connect.sh:ro
    entrypoint: [ "sh", "/connect.sh" ]
    depends_on:
      - connect

  mysql:
    image: mysql:8.0.16
    container_name: mysql
    restart: always
    ports:
      - 6033:3306
    command: --init-file /mysql_init.sql
    volumes:
        - ./init/mysql/mysql_init.sql:/mysql_init.sql
    environment:
      - MYSQL_ROOT_PASSWORD=password

  phpmyadmin:
    image: phpmyadmin:5.1.3
    container_name: phpmyadmin
    restart: always
    ports:
      - 8082:80
    links:
      - mysql
    environment:
      PMA_HOST: mysql
      PMA_PORT: 3306
      PMA_ARBITRARY: 1
      UPLOAD_LIMIT: 600M
